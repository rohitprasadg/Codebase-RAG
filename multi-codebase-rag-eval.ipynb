{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from git import Repo\n",
    "from openai import OpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from rouge_score.rouge_scorer import RougeScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLONE_DIR = r\"C:\\Users\\Rohit\\Documents\\MyProjects\\codebase_rag\"  # repos will be stored here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_EXTENSIONS = [\".py\", \".js\", \".tsx\", \".ts\", \".java\", \".cpp\", \".yml\"]\n",
    "\n",
    "IGNORED_DIRS = [\".git\", \"node_modules\", \"dist\", \"__pycache__\", \".next\", \".vscode\", \".env\", \"venv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clones repo if not exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repo(repo_url):\n",
    "    \"\"\"Clone a repository and return its local path\"\"\"\n",
    "    repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
    "    repo_path = os.path.join(CLONE_DIR, repo_name)\n",
    "    \n",
    "    if not os.path.exists(repo_path):\n",
    "        Repo.clone_from(repo_url, repo_path)\n",
    "        print(f\"Cloned {repo_name} to {repo_path}\")\n",
    "    else:\n",
    "        print(f\"Repository {repo_name} already exists at {repo_path}\")\n",
    "    \n",
    "    return repo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below reads a file's content and returns the relative path and content in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_content(file_path, repo_path, repo_name):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            content = f.read()\n",
    "        rel_path = os.path.relpath(file_path, repo_path)\n",
    "        return {\n",
    "            \"repo\": repo_name,\n",
    "            \"name\": rel_path,\n",
    "            \"content\": content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provides a way to gather the contents of supported code files in a repository and returns a list of dictionaries where each dictionary contains the relative path of the file and the contents of the file. (EXTRACTS CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_files_content(repo_path, repo_name):\n",
    "    files_content = []\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(repo_path):\n",
    "            # Skip ignored directories\n",
    "            dirs[:] = [d for d in dirs if d not in IGNORED_DIRS]\n",
    "            \n",
    "            for file in files:\n",
    "                if os.path.splitext(file)[1] in SUPPORTED_EXTENSIONS:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_content = get_file_content(file_path, repo_path, repo_name)\n",
    "                    if file_content:\n",
    "                        files_content.append(file_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {repo_name}: {str(e)}\")\n",
    "    \n",
    "    return files_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_codebases(codebase_inputs):\n",
    "    \"\"\"Process multiple codebases (either URLs or local paths)\"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    for input_path in codebase_inputs:\n",
    "        if input_path.startswith(\"http\"):\n",
    "            # It's a repository URL - clone it\n",
    "            repo_path = clone_repo(input_path)\n",
    "            repo_name = os.path.basename(repo_path)\n",
    "        else:\n",
    "            # It's a local path - use directly\n",
    "            repo_path = input_path\n",
    "            repo_name = os.path.basename(repo_path)\n",
    "        \n",
    "        # Get files from this codebase\n",
    "        files_content = get_main_files_content(repo_path, repo_name)\n",
    "        all_files.extend(files_content)\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File content > Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(files_content):\n",
    "    documents = []\n",
    "    for file in files_content:\n",
    "        doc = Document(\n",
    "            page_content=f\"REPO: {file['repo']}\\nFILE: {file['name']}\\nCONTENT:\\n{file['content']}\",\n",
    "            metadata={\n",
    "                \"repo\": file['repo'],\n",
    "                \"source\": file['name']\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store embeddings in chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vector_store(documents):\n",
    "    return Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=\"multi-codebase-rag\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Codebase inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODEBASE_INPUTS = [\n",
    "    \"https://github.com/langchain-ai/langchain.git\",\n",
    "    \"https://github.com/evershopcommerce/evershop.git\",\n",
    "    \"https://github.com/atinder11/Youtube-Clone.git\"\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = process_codebases(CODEBASE_INPUTS)\n",
    "documents = create_documents(all_files)\n",
    "vectorstore = create_vector_store(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does the Stripe payment module work?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top 5 docs\n",
    "relevant_docs = vectorstore.similarity_search(query=query, k=5)\n",
    "contexts = [doc.page_content for doc in relevant_docs]\n",
    "formatted_context = \"\\n\\n-------\\n\\n\".join(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"Answer questions using information from these codebases:\n",
    "    {context}\n",
    "    \n",
    "    Guidelines:\n",
    "    - Specify which repository (REPO) the information comes from\n",
    "    - If information comes from multiple repos, note that.\n",
    "    - If unsure, say which repos you checked.\n",
    "    - Don't invent anything not in the context.\n",
    "    - Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_TEMPLATE),\n",
    "        (\"human\", \"Question: {question}\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm  #The pipe operator (|) is used here to create a “chain” that connects the prompt template with the language model.\n",
    "response = chain.invoke({\"context\": formatted_context, \"question\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "From the REPO: evershop, the Stripe payment module includes several functionalities to manage Stripe payments. It uses a webhook to handle Stripe payment events, capturing, authorizing, and updating payment statuses within the Evershop database accordingly. Additionally, the system incorporates order processing logic that updates order statuses and emits events based on Stripe transaction results, integrates settings for Stripe configurations, and implements error handling to manage failed payment transactions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnswer:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opens and reads the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rag_eval_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the expected response from the dataset based on the user's query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_expected_response(user_query):\n",
    "    for query in rag_eval_data[\"queries\"]:\n",
    "        if query[\"question\"].lower() == user_query.lower():\n",
    "            return query\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== QUERY EVALUATION =====\n",
      "\n",
      "User Query: How does the Stripe payment module work?\n",
      "Source Repository: evershop\n",
      "\n",
      "EXPECTED RESPONSE:\n",
      "The Stripe payment module in the Evershop repository integrates Stripe for processing payments. It retrieves and validates payment intents, updates payment status based on Stripe transaction outcomes, captures payment details, and handles webhook events for payment statuses such as succeeded, capturable, and canceled. It supports both capture and authorization-only modes.\n",
      "\n",
      "LLM RESPONSE:\n",
      "The Stripe payment module in the Evershop repository provides functionalities for payment processing using Stripe. It allows capturing and handling Stripe payments through webhooks, setting up payment methods, and managing payment statuses based on Stripe transactions. The module includes mechanisms for updating order statuses based on Stripe payment outcomes, capturing payment intents, and handling Stripe events post-payment such as cancellations and completions, thus ensuring the e-commerce transactions are properly synchronized with Stripe's payment processing flows.\n",
      "BLEU-1 Score: 46.43%\n",
      "ROUGE-1 Precision: 48.72%\n",
      "ROUGE-1 Recall: 74.51%\n",
      "ROUGE-1 F1: 58.91%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_rag_system(user_query):\n",
    "\n",
    "    expected_data = find_expected_response(user_query)\n",
    "    \n",
    "    if not expected_data:\n",
    "        print(\"No matching question found in the dataset.\")\n",
    "        return\n",
    "    \n",
    "    expected_response = expected_data[\"expected_response\"]\n",
    "    source = expected_data[\"source\"]\n",
    "\n",
    "    \n",
    "    relevant_docs = vectorstore.similarity_search(query=user_query, k=5)\n",
    "    contexts = [doc.page_content for doc in relevant_docs]\n",
    "    formatted_context = \"\\n\\n-------\\n\\n\".join(contexts)\n",
    "\n",
    "    \n",
    "    response = chain.invoke({\"context\": formatted_context, \"question\": user_query})\n",
    "    llm_response = response.content\n",
    "\n",
    "    bleu_score = 0.0\n",
    "    rouge1 = {'precision': 0.0, 'recall': 0.0, 'fmeasure': 0.0}\n",
    "    \n",
    "    try:\n",
    "        expected_clean = expected_response.lower()\n",
    "        llm_clean = llm_response.lower()\n",
    "        \n",
    "        # Tokenize/ split sentence to words\n",
    "\n",
    "        expected_tokens = nltk.word_tokenize(expected_clean)\n",
    "        llm_tokens = nltk.word_tokenize(llm_clean)\n",
    "        \n",
    "        # About Bleu-1:\n",
    "        # BLEU-1 (Bilingual Evaluation Understudy) is a metric used to measure how similar an AI response is to a expected response.\n",
    "        # It calculates how many individual words (unigrams) from the expected response appear in the AI-generated response.\n",
    "\n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu(\n",
    "            [expected_tokens], \n",
    "            llm_tokens, \n",
    "            weights=(1, 0, 0, 0)         # meaning-only single word matches are considered.\n",
    "        ) * 100\n",
    "        \n",
    "        # ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures how many words from the expected response appear in the generated response. \n",
    "\n",
    "        scorer = RougeScorer(['rouge1'], use_stemmer=True)\n",
    "        rouge_scores = scorer.score(expected_clean, llm_clean)\n",
    "        rouge1 = rouge_scores['rouge1']\n",
    "        \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error calculating NLP metrics: {e}\")\n",
    "        print(\"Trying fallback tokenization...\")\n",
    "\n",
    "        expected_tokens = expected_clean.split()\n",
    "        llm_tokens = llm_clean.split()\n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu(\n",
    "            [expected_tokens], \n",
    "            llm_tokens, \n",
    "            weights=(1, 0, 0, 0)\n",
    "        ) * 100 if expected_tokens else 0.0\n",
    "\n",
    "    #################\n",
    "    # PRINTING RESULTS\n",
    "    #################\n",
    "\n",
    "    print(\"\\n===== QUERY EVALUATION =====\\n\")\n",
    "    print(f\"User Query: {user_query}\")\n",
    "    print(f\"Source Repository: {source}\")\n",
    "    print(\"\\nEXPECTED RESPONSE:\")\n",
    "    print(expected_response)\n",
    "    print(\"\\nLLM RESPONSE:\")\n",
    "    print(llm_response)\n",
    "    print(f\"BLEU-1 Score: {bleu_score:.2f}%\")\n",
    "    print(f\"ROUGE-1 Precision: {rouge1.precision * 100:.2f}%\")\n",
    "    print(f\"ROUGE-1 Recall: {rouge1.recall * 100:.2f}%\")\n",
    "    print(f\"ROUGE-1 F1: {rouge1.fmeasure * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Precision = matching words/ words in generated response\n",
    "\n",
    "Recall = matching words/ words in expected response\n",
    "\n",
    "F1 = 2*P*R/(P+R)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_query = \"How does the Stripe payment module work?\"\n",
    "evaluate_rag_system(user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
